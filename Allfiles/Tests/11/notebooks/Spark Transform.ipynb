{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transform data by using Spark\n",
        "\n",
        "This notebook transforms sales order data; converting it from CSV to Parquet format and splitting customer name into two separate fields.\n",
        "\n",
        "## Set variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Load source data\n",
        "\n",
        "Let's start by loading some historical sales order data into a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from azure.storage.blob import BlobServiceClient, BlobClient\n",
        "import zipfile\n",
        "# Create a connection to your Azure Blob Storage account\n",
        "blob_service_client = BlobServiceClient.from_connection_string('<your-connection-string>')\n",
        "\n",
        "# Specify the name of your container and the path to the zip file\n",
        "container_name = 'files'\n",
        "zip_file_path = 'enrollmentdb'\n",
        "\n",
        "import datetime\n",
        "# Generate a unique folder name using a timestamp\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "folder_name = f\"converteddata_{timestamp}\"\n",
        "\n",
        "# Specify the path where to extract the contents of the zip file\n",
        "extract_path = f\"{folder_name}/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the zip file to a local temporary file\n",
        "local_zip_path = 'temp.zip'\n",
        "blob_client = blob_service_client.get_blob_client(container=container_name, blob=zip_file_path)\n",
        "with open(local_zip_path, 'wb') as file:\n",
        "    file.write(blob_client.download_blob().readall())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Transform the data structure\n",
        "\n",
        "The source data includes a **CustomerName** field, that contains the customer's first and last name. Modify the dataframe to separate this field into separate **FirstName** and **LastName** fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Iterate over the extracted files\n",
        "for file_name in zip_ref.namelist():\n",
        "    # Skip directories\n",
        "    if not file_name.endswith('/'):\n",
        "        # Check if the file has the extension '.mdb' or '.accdb'\n",
        "        if file_name.endswith('.mdb') or file_name.endswith('.accdb'):\n",
        "            # Read the Microsoft Access database file into a pandas DataFrame\n",
        "            access_file_path = f'{extract_path}/{file_name}'\n",
        "            df = pd.read_csv(access_file_path, delimiter=';')\n",
        "\n",
        "            # Specify the path to save the CSV file\n",
        "            csv_file_path = f'{extract_path}/{file_name}.csv'\n",
        "\n",
        "            # Save the DataFrame as a CSV file\n",
        "            df.to_csv(csv_file_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Save the transformed data\n",
        "\n",
        "Now save the transformed dataframe in Parquet format in a folder specified in a variable (Overwriting the data if it already exists)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Upload the converted CSV file to Azure Blob Storage\n",
        "        blob_name = f'{folder_name}/{file_name}.csv'\n",
        "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
        "        with open(csv_file_path, \"rb\") as data:\n",
        "            blob_client.upload_blob(data)\n",
        "\n",
        "        # Clean up the local CSV file\n",
        "        os.remove(csv_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up the extracted folder\n",
        "shutil.rmtree(extract_path)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
